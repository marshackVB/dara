# Workspace authentication
DATABRICKS_URL =
DATABRICKS_CLIENT_ID =
DATABRICKS_CLIENT_SECRET =

# The backend task store that will be used
TASK_STORE_BACKEND = lakebase # Options are lakebase or redis though you can add you own backend.

# Task store authentication credentials
# If using Databricks Lakebase
LAKEBASE_HOST =
LAKEBASE_DB_NAME =
LAKEBASE_INSTANCE_NAME =
LAKEBASE_PORT =
LAKEBASE_ROLE =
LAKEBASE_PASSWORD =
# The schema and table where the task store will be created (if it doesn't exist)
TASK_STORE_POSTGRES_SCHEMA =
TASK_STORE_POSTGRES_TABLE =
TASK_STORE_TTL_SECONDS = 86400 # 24 hour default
TASK_STORE_POSTGRES_POOL_SIZE = 10 # default


# If using Redis (note: Change TASK_STORE_BACKEND to 'redis'):
#REDIS_HOST = 
#REDIS_PORT = 
#REDIS_PASSWORD = 


# To set up the example Databricks workflow
# Files will be saved in the below location
#CATALOG =
#SCHEMA =
#VOLUME =

# The ID of the Databricks Job that will process the file
#JOB_ID =


# Optional cluster id if using Spark connect for local development
#DATABRICKS_CLUSTER_ID = 
